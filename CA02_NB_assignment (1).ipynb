{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm.\n",
        "\n",
        "In this assignment you will ...\n",
        "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
        "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
        "\n",
        "IMPORTANT NOTE:\n",
        "\n",
        "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Import all other necessary libraries. Your code below ...\n",
        "from sklearn.naive_bayes import GaussianNB ## This module facilitates the training of a Gaussian Naive Bayes model using features (x) and target (y), and it includes a prediction function.\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics # Importing the metrics module from scikit-learn for evaluating and measuring model performance.\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def make_Dictionary(root_dir): # Create a function that accepts a directory path as an argument\n",
        "\n",
        "  all_words = []                                                        # 1. Create an empty list to store all the words extracted from our email inputs.\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    # 2. Retrieve each file name in the specified folder (e.g., train- and test-email folders) using the os module for file and directory operations.\n",
        "\n",
        "  for mail in emails:               # 1. Use a for loop to iterate over each email file, equivalent to saying \"for each mail in our emails.\"\n",
        "    with open(mail) as m:           # 2. The \"open()\" function opens a file, and \"as m\" renames it as 'm'. The \"open()\" function takes three arguments: open(filename, mode, encoding).\n",
        "      for line in m:                # 3. Use a for loop to iterate over each line in opened email (iterate over each line, one email at a time).\n",
        "        words = line.split()        # 4. Divide each line into individual words and store them in the variable \"words.\"\n",
        "        all_words += words          # 5. Gathers all the recorded \"words\" and compiles them into a list named \"all_words.\"\n",
        "\n",
        "  dictionary = Counter(all_words)     # 1. Utilizing the Counter() function from the collections module, the \"all_words\" list is converted into a dictionary. Specifically, each distinct word becomes a key, and its occurrences are recorded as the corresponding value in the key-value pair\n",
        "  list_to_remove = list(dictionary)   # 2. Change the dictionary into a list to facilitate iteration, enabling the elimination of undesirable characters and words\n",
        "\n",
        "\n",
        "  for item in list_to_remove:     # 1. Utilize a for loop to traverse through each element in the recently created list.\n",
        "    if item.isalpha() == False:   # 2. The \".isalpha()\" method verifies whether the item comprises exclusively alphabetical letters. It yields True for alphabetic items and False otherwise.\n",
        "      del dictionary[item]        # 3. As we're inspecting for non-alphabetical items (as determined in the line above), deletion of the item occurs only when it includes a non-alphabetical character.\n",
        "    elif len(item) == 1:          # 4. When the item comprises only alphabetical characters, we check if it is a single character\n",
        "      del dictionary[item]        # 5. If it happens to be a single character, we proceed to remove it.\n",
        "\n",
        "\n",
        "  dictionary = dictionary.most_common(3000)   # 1. The \".most_common()\" function, part of the collections library, retains solely the top 3,000 (as specified) most frequently occurring words in the dictionary based on their respective frequencies.\n",
        "  return dictionary                           # 2. Provide the dictionary as the output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def extract_features(mail_dir):       # Create a function that takes a directory path as its input.\n",
        "\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]    # 1. Utilizing \"os.path.join()\", this function intelligently concatenates multiple paths using \"/\" as a separator. For more information, refer to the documentation: https://www.geeksforgeeks.org/python-os-path-join-method/\n",
        "  features_matrix = np.zeros((len(files),3000))                       # 2. In this context, \"features\" exclusively denotes Independent Variables (IVs) and excludes the target (Dependent Variable, DV). Utilizing the \"np.zeros\" function, an array is created with a 2D shape determined by the length of files and 3,000, filled with zeros. More information can be found in the documentation: https://www.geeksforgeeks.org/numpy-zeros-python/\n",
        "  train_labels = np.zeros(len(files))                                 # 3. Using a single argument for \"np.zeros\" results in a 1D array filled with zeros, assigning train_labels as such. In this setup, 0 signifies NOT SPAM, while 1 denotes SPAM.\n",
        "  count = 1;                                                          # 4. Set up a counter variable named \"count\" with an initial value.\n",
        "  docID = 0;\n",
        "\n",
        "  for fil in files:                                                     # 1. Use a for loop with the variable \"fil\" to iterate through each intelligently joined file in the \"files\" collection.\n",
        "    with open(fil) as fi:                                               # 2. The \"open()\" function is used to open a file, and it is given the alias 'fi'. The \"open()\" function takes three arguments: filename, mode, an\n",
        "      for i, line in enumerate(fi):                                     # 3. Go through each line in the file, keeping track of the line number (index i) and the actual content of the line (line).\n",
        "        if i ==2:                                                       # 4. Check if the current line\n",
        "          words = line.split()                                          # 5. Split the line into individual words, employing whitespace as the separator.\n",
        "          for word in words:                                            # 6. Iterate over each word in the list of words\n",
        "            wordID = 0                                                  # 7. Create a variable to store the ID of the current word in the dictionary\n",
        "            for i, d in enumerate(dictionary):                          # 8. Loop through the dictionary using enumerate to access the index (i) and the word-frequency pair (d)\n",
        "              if d[0] == word:                                          # 9. Verify if the word in the dictionary matches the current word\n",
        "                wordID = i                                              # 10. If there's a match, set wordID to the index in the dictionary\n",
        "                features_matrix[docID,wordID] = words.count(word)       # 11. Update the feature matrix with the count of the current word in the current document\n",
        "\n",
        "      train_labels[docID] = 0;                            # Initialize the label for the current document as 0\n",
        "      filepathTokens = fil.split('\\\\')                     # Split the file path into tokens using the '/' separator\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]   # Get the last token from the file path\n",
        "      if lastToken.startswith(\"spmsg\"):                   # Check if the last token starts with \"spmsg\"\n",
        "        train_labels[docID] = 1;                          # If it does, set the label for the current document to 1\n",
        "        count = count + 1                                 # Increment the count\n",
        "      docID = docID + 1                                   # Increment the document ID for the next iteration\n",
        "  return features_matrix, train_labels                    # Return the resulting feature matrix and labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1quCpnVSlXZD",
        "outputId": "2c14051d-068d-4bd6-efd0-c97e0b4b1ace"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "TRAIN_DIR = r\"/content/drive/MyDrive/Data/train-mails\"\n",
        "TEST_DIR = r\"/content/drive/MyDrive/Data/test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "3fa3683e-7062-45ef-f03c-287733ead62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from files.\n"
          ]
        }
      ],
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from files.\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mC9DyW1AGCUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd56b38-eda1-4bbe-e382-ad62881f27e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training Model Gaussian Naibe Bayes Algorithm...\n",
            "Trainin completed\n",
            "testing trained model to predict test data labes... \n",
            "testing completed\n",
            "Accuracy of the model is:  1.0\n"
          ]
        }
      ],
      "source": [
        "# In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
        "# Your code below ...\n",
        "# Create an instance of a Gaussian Naive Bayes classifier and assign it to nb_classifier\n",
        "# instantiate a Gaussian NB classifier = nb_classifier\n",
        "\n",
        "    # 8 SPAM emails are incorrectly labeled as NOT SPAM whereas 1 NOT SPAM is incorrectly labeled as SPAM\n",
        "    # Display the confusion matrix in table format\n",
        "\n",
        "\n",
        "\n",
        "# Creating and training a Gaussian Naive Bayes model\n",
        "email_spam_model = GaussianNB()\n",
        "train = email_spam_model.fit ( features_matrix, labels)\n",
        "print(\"training Model Gaussian Naibe Bayes Algorithm...\")\n",
        "print(\"Trainin completed\")\n",
        "\n",
        "\n",
        "# Making predictions on the test set\n",
        "test = email_spam_model.predict(test_features_matrix)\n",
        "print(\"testing trained model to predict test data labes... \")\n",
        "print(\"testing completed\")\n",
        "\n",
        "\n",
        "# Calculating and displaying accuracy\n",
        "accuracy = accuracy_score ( test_labels, test)\n",
        "print(\"Accuracy of the model is: \", accuracy)\n",
        "\n",
        "\n",
        "# Your output should look like below if your code is right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}